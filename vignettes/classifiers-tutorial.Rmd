---
title: "Acoustic classifiers"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Acoustic classifiers}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(ggplot2)
library(httr2)
library(readr)
library(wildrtrax)

load("classifiers.RData")
#save.image("package.RData")
#save.image("classifiers.RData")
```

# Deep learning in acoustic processing

Recent advances in deep learning have led to the development of neural network models that can classify the sounds within acoustic recordings, such as those captured by autonomous recording units (ARUs). These classifiers can be trained to detect just a single focal species, or to classify thousands of species. The process of using automated classifiers to extract species detections from acoustic recordings is collectively called "computer listening". This tutorial will show you how to access and work with classifier results for recordings in WildTrax.

## Classifiers
Classifier scores can be converted to species detections by setting a threshold (e.g., 0.8) above which to consider a species present within a given spectrogram @wood_2024. False positives can still occur at high score thresholds, so often verification by a human observer is still necessary.


[BirdNET](https://birdnet.cornell.edu/) is a deep learning classifier developed by the Cornell Lab of Ornithology that is trained to classify more than 6,000 of the world's most common bird species, including most North American bird species @kahl_2022. The model converts audio recordings into windows of spectrograms and outputs a probability score for each species in each one. The WildTrax 2.0 release (2025) is accompanied by the introduction of a new deep learning model, HawkEars, that can classify many of Canada's most common bird species. HawkEars is implemented in the same fashion as BirdNET so that users will also be able to download a report and use the same set of `wildrtrax` functions demonstrated above on it. HawkEars is also freely available from [Github](https://www.github.com/jhuus/HawkEars). Initial tests of HawkEars on the same expert dataset as above suggest it performs much better than BirdNET for Canadian species, with more than double the recall and higher precision at score thresholds above 50. 

## Classifier performance

Choosing a score threshold will depend on the goals of the project; however, threshold choice is a trade-off between false positives (i.e., incorrect classifications) and false negatives (i.e., missed detections; @priyadarshani_2018, @knight_2017). Choosing a high score threshold will minimize false positives, but will also result in false negatives. Choosing a low score threshold will minimize false negatives but will result in many false positives. The proportion of false positives at a given score threshold is typically measured by precision:

$precision = \frac{tp}{tp + fp}$

While the proportion of false negatives is measured as recall:

$recall = \frac{tp}{tp + fn}$

Where *tp* is the number of true positives, *fp* is the number of false positives, and *fn* is the number of false negatives.

The threshold-agnostic performance of a classifier is then typically evaluated as the area under the curve (AUC) of a precision-recall curve. The corner of the precision recall curve can be used to select a score threshold.

F-score is a combination of precision and recall and can also used to select a score threshold by selecting the peak value.

$Fscore = \frac{2 * precision* recall}{precision + recall}$

ABMI has evaluated BirdNET with a dataset of 623 3-minute recordings. All species were annotated in each minute of each recording by our top expert listeners and further groomed for false positives and negatives. The dataset was selected to include at least 10 recordings with detections of the most common 203 Canadian bird species. Recordings were primarily sourced from Alberta and Ontario to include variation in dialect. We evaluated BirdNET by running it using the local eBird occurrence data for each recording and comparing results with our expert dataset and pooling the total detections across species per minute of recording to calculate overall precision, recall, and F-score.

Precision ranged from 0.36 at a score threshold of 0.10 to 0.94 at a score threshold of 0.99 (Figure 1). Recall ranged from 0.01 at a score threshold of 0.99 to 0.36 of 0.1  F-score was similarly low, ranging from 0.03 at a score threshold of 0.01 to 0.36 at a score threshold of 0.99. Neither the precision-recall curve nor the plot of F-score relative to score threshold showed a typical concave down curve shape, suggesting that a low score threshold of 0.10 would be best to optimize trade-offs between precision and recall.

```{r, eval=T, include=T, echo=F, message=F, warning=F}

tmp_file <- tempfile(fileext = ".csv")

request("https://raw.githubusercontent.com/ABbiodiversity/wildRtrax-assets/main/ExpertData_PR_Total.csv") |>
  req_perform(path = tmp_file)

# Read and filter
dat <- read_csv(tmp_file) |>
  filter(classifier == "BirdNET", thresh >= 0.1)

```

```{r, eval=T, include=T, echo=F, message=F, warning=F}

plot.f <- ggplot(dat) +
  geom_line(aes(x=thresh, y=f), size=1.5) +
  xlab("Score threshold") +
  ylab("F-score") +
  xlim(c(0.1, 1)) +
  ylim(c(0, 1)) +
  theme_bw()

plot.f

```

```{r, eval=T, include=T, echo=F, message=F, warning=F}
Sys.setenv(WT_USERNAME = 'guest', WT_PASSWORD = 'Apple123')
wt_auth()

data <- wt_download_report(620, 'ARU', c('main','ai'))

eval <- wt_evaluate_classifier(data, resolution = "task", remove_species = T, species = NULL, thresholds = c(0.25,0.99))

plot.pr.e <- ggplot(eval) +
  geom_smooth(aes(x=recall, y=precision, colour=classifier), linewidth=1.5) +
  xlab("Recall") +
  ylab("Precision") +
  xlim(0,1) +
  ylim(0,1) +
  theme_bw()

plot.pr.e

```

WildTrax uses both BirdNET and HawkEars to automatically classify species in all recordings that are uploaded to the system. The sensitivity for BirdNET is set at 1.5 to reduce the probability of false positives and the score threshold is set low at 0.1 to allow users to set higher thresholds as needed. The list of species is filtered by eBird occurrence data for the week of recording, but not by location.

## Downloading the classifier reports

Use `wt_download_report(reports = c('main','ai'))` to download the classifier reports and the main report for further analysis. We'll use the OSM 2023 dataset as an example.

```{r, include=F, message=F, warning=F, echo=F, eval=T}
Sys.setenv(WT_USERNAME = 'guest', WT_PASSWORD = 'Apple123')
wt_auth()

# Download the project. Here's a recent ABMI project accessible as Map + Report Only.
data <- wt_download_report(project_id = 2088,
                           sensor_id = "ARU",
                           reports = c("main", "ai"), 
                           weather_cols = FALSE)

```

```{r, include=T, eval=F}

Sys.setenv(WT_USERNAME = 'guest', WT_PASSWORD = 'Apple123')
wt_auth()

#This line will take a minute to run while it downloads the data
data <- wt_download_report(project_id = 2088,
                           sensor_id = "ARU",
                           reports = c("main", "ai"), 
                           weather_cols = FALSE)

```

## Evaluating

We can combine the `main` report and the `ai` report to evaluate the classifier's performance on a given dataset. The `wt_evaluate_classifier()` function takes the output from the `wt_download_report()` and joins them together and then calculates precision, recall, and F-score for the requested sequences of thresholds. You can request the metrics at the minute level for recordings that are processed with the species per minute method (1SPM). You can also exclude species that are not allowed in the project from the BirdNET results before evaluation.

```{r, message=F, warning=F}

eval <- wt_evaluate_classifier(data,
                              resolution = "task",
                              remove_species = TRUE,
                              thresholds = c(0.25, 0.99)) # 0.25 is the minimum score threshold offered from the reports.

tail(eval, 5)
```

We can plot the results of our evaluation to get an idea of how each classifier is performing:

```{r, message=F, warning=F}
plot.p.e <- ggplot(eval) +
  geom_line(aes(x=threshold, y=precision, colour=classifier), linewidth=1.5) +
  xlab("Score threshold") +
  ylab("Precision") +
  xlim(0,1) +
  ylim(0,1) +
  theme_bw()

plot.r.e <- ggplot(eval) +
  geom_line(aes(x=threshold, y=recall, colour=classifier), linewidth=1.5) +
  xlab("Score threshold") +
  ylab("Recall") +
  xlim(0,1) +
  ylim(0,1) +
  theme_bw()

plot.f.e <- ggplot(eval) +
  geom_line(aes(x=threshold, y=fscore, colour=classifier), linewidth=1.5) +
  xlab("Score threshold") +
  ylab("F-score") +
  xlim(0,1) +
  ylim(0,1) +
  theme_bw()

plot.pr.e <- ggplot(eval) +
  geom_line(aes(x=recall, y=precision, colour=classifier), linewidth=1.5) +
  xlab("Recall") +
  ylab("Precision") +
  xlim(0,1) +
  ylim(0,1) +
  theme_bw()

par(mfrow = c(2, 2))

print(plot.p.e)
print(plot.r.e)
print(plot.f.e)
print(plot.pr.e)

par(mfrow = c(1, 1))  # reset
```

## Selecting and filtering a threshold

You can use the precision and recall values in the output of the `wt_evaluate_classifier()` function to select a score threshold manually, or you can use the `wt_classifier_threshold()` function to select the highest threshold that maximizes F-score.

```{r, message=F, warning=F}

wt_classifier_threshold(eval)

```

Once a threshold has been selected, the report can be then be filtered as desired. Whether you're a human or a computer, all classifiers make mistakes. But we can select a score threshold that maximizes the F-score. Let's look at what our precision is:

```{r, message=F, warning=F}

eval[eval$threshold==0.75,]

```

At our chosen score threshold, the classifiers show very different behaviours. BirdNET v2.1 and BirdNET-Lite have high precision but extremely low recall, meaning they miss the vast majority of true detections. In contrast, HawkEars v1.0.8 achieves a more balanced performance: its precision is moderate but its recall is substantially higher. This means HawkEars identifies more real events, though many false positives still require visual verification. From a detectability perspective, recall values in the 10â€“35% range indicate that automated detection yields only a fraction of the detections that a human listener would capture, so all model outputs should still be interpreted with caution when used in ecological analyses.

## Check for additional species detected

One of the potential valuable applications of these classifiers is to check for the presence of additional species in acoustic recordings that were not detected by human listeners. @ware_2023 found that supplementing human listener data with verified computer listening results improved estimates of species richness, particularly for water-associated birds. We can use the `wt_additional_species()` function to check for species reported by each classifier that the human listeners did not detect in our project. The input for this function should be the output from the `wt_download_report()` function when you request the `main` and `ai` reports and you will need to set a score threshold.

Let's use a high threshold (80) on our example dataset to see if any new species are detected. We can use the resolution argument to specify whether we want to look for new species in each task, recording, location, or in the entire project. Let's pretend we're interested in site-specific species richness and use the task argument.

```{r, eval=F, message=F, warning=F}

new <- wt_additional_species(data, remove_species = TRUE, threshold = 80, resolution="task")

#potential new detections
nrow(new)

table(new$species_code)

```

There are no potential new species detections in our dataset. Which is good! The humans found all the same species as the classifiers.

<span style="display: inline-block; border: 1px solid #000; padding: 5px; background-color: #f0f0f0;">
If no human-generated tags exist in a project, in other words you are only using classifiers to detect species, additional tags can be easily synchronized with `wt_additional_tags(format_to_tags = TRUE)` along with an output folder.
If you are adding additional tags to a human processed data set, the best approach is sync the tags onto new tasks. Generate tasks with the 'Not Assigned' observer and then sync the output of `wt_additional_tags()`. The common error you may encounter are when there are either conflicts for the number of individuals assigned relative to the task method.
</span>.

## Individual calls

Another potential use for BirdNET and HawkEars in WildTrax is to use it to detect individual calls as opposed to just the first call in each task (1SPT) or minute (1SPM). This might be of interest if you're using call rate in a behavioural analysis, or if you're looking for detections for tool development like distance estimation or building a focal species recognizer. Let's try it for White-throated Sparrow (WTSP):

```{r, eval=F, message=F, warning=F}

#Evaluate classifier performance
eval_wtsp <- wt_evaluate_classifier(data,
                              resolution = "task",
                              remove_species = TRUE,
                              species = "White-throated Sparrow",
                              thresholds = c(0.25, 0.99))

#Filter the detections to the best threshold
threshold_wtsp <- wt_classifier_threshold(eval_wtsp)

#Look at performance at that threshold
eval_wtsp[eval_wtsp$threshold==min(threshold_wtsp$threshold),]

#Filter to detections
detections_wtsp <- data[[1]] |>
  filter(species_common_name == "White-throated Sparrow", 
         confidence > min(threshold_wtsp$threshold))


```

As before, you'll probably want to upload your detections to WildTrax for verification, even though the classifiers performance for White-throated Sparrow is pretty good. Let's take a look at our output as call rate to see if it's higher at the beginning of the season, as we would expect:

```{r, eval=F, include=T, message=F, warning=F}
library(lubridate)

#Calculate detections per second and mean confidence in each recording
rate_wtsp <- detections_wtsp |> 
  group_by(location_id, recording_date_time, recording_length, version) |>
  summarize(calls = n(),
            confidence = mean(confidence),
            .groups = "keep") |> 
  ungroup() |> 
  mutate(rate = calls/recording_length*60,
         recording_date_time = ymd_hms(recording_date_time),
         yday = yday(recording_date_time),
         hour = hour(recording_date_time))

#Filter to the sites with most recordings with detections
occupied_wtsp <- rate_wtsp |> 
  group_by(location_id) |> 
  mutate(recordings = n()) |> 
  ungroup() |> 
  dplyr::filter(recordings >= 4)

#Plot call rate by day of year
ggplot(occupied_wtsp) + 
  geom_point(aes(x=yday, y=rate)) +
  geom_smooth(aes(x=yday, y=rate)) +
  xlab("Day of year") +
  ylab("Rate of Clay-coloured sparrow detections per minute") +
  theme_bw()

```

## Other applications

Visit the [BirdNET Github repository](https://github.com/kahst/BirdNET-Analyzer) and HawkEars repository to run or modify these classifiers on your own computer. The decision to pursue other applications should be made with the effect of a classifier's low recall rate in mind:

1. With presence / absence data, a classifier is unlikely to be reliably to confirm absences due to the low recall.

2. Classifier data can be used for occupancy modelling (@wood_2023), and there are approaches that can accommodate false positive error rates to preclude verification of all detections (@rhinehart_2022). However, users should keep in mind that occupancy modelling is recommended only for detection probabilities > 30% and that recall from BirdNET may be too low for reliable occupancy estimates for many species (@knight_2017).

See @perez-granados_2023 for a full review of some classifier applications and performance.
